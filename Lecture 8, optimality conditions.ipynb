{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8, Optimality conditions and SQP methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are still studying the full problem\n",
    "\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## What aspects are important for optimality conditions?\n",
    "* Think about this for a while\n",
    "* You can first think about the case without constraints and, then, what should be added there\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to identify which points are optimal, we want to define similar conditions as there are for unconstrained problems through the gradient:\n",
    "\n",
    ">If $x$ is a  local optimum to function $f$, then $\\nabla f(x)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KKT conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem (First order Karush-Kuhn-Tucker (KKT) Necessary Conditions)** \n",
    "\n",
    "Let $x^*$ be a local minimum for problem\n",
    "$$\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us assume that objective and constraint functions are continuosly differentiable at a point $x^*$ and assume that $x^*$ satisfies some regularity conditions (see e.g., https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_.28or_constraint_qualifications.29 ). Then there exists unique Lagrance multiplier vectors $\\mu^*=(\\mu_1^*,\\ldots,\\mu_J^*)$ and $\\lambda^* = (\\lambda^*_1,\\ldots,\\lambda_K^*)$ such that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x^*,\\mu^*,\\lambda^*) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J\\\\\n",
    "&\\mu_j^*g_j(x^*)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $L$ is the *Lagrangian function* $$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## An example of constraint qualifications for inequality constraint problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Definition (regular point)**\n",
    "\n",
    "A point $x^*\\in S$ is *regular* if the set of gradients of the active inequality constraints \n",
    "\n",
    "$$\n",
    "\\{\\nabla g_j(x^*) | \\text{ constraint } i \\text{ is active}\\}\n",
    "$$\n",
    "\n",
    "is linearly independent. This means that none of them can be expressed as a linear combination of the others. (*In a simple language one might say that they point to different directions; as an example you can think of the basis vectors of $\\mathbb R^n$*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KKT conditions were developed independently by \n",
    "* Karush:\"Minima of Functions of Several Variables with Inequalities as Side Constraints\". *M.Sc. Dissertation*, Dept. of Mathematics, Univ. of Chicago, 1939\n",
    "* Kuhn & Tucker: \"Nonlinear programming\", In: *Proceedings of 2nd Berkeley Symposium*, pp. 481â€“492, 1951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The coefficients $\\mu$ and $\\lambda$ are called the *KKT multipliers*.\n",
    "\n",
    "The first equality \n",
    "\n",
    "$$\n",
    "\\nabla_xL(x,\\mu,\\lambda) = 0\n",
    "$$\n",
    "\n",
    "is called the stationary rule and the requirement \n",
    "\n",
    "$$\n",
    "\\mu_j^*g_j(x)=0,\\text{for all }j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "is called the complementarity rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Consider the optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\qquad (x_1^2+x^2_2+x^2_3)\\\\\n",
    "\\text{s.t}&\\qquad x_1+x_2+x_3-3\\geq 0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us verify the KKT necessary conditions for the local optimum $x^*=(1,1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can see that\n",
    "\n",
    "$$\n",
    "L(x,\\mu,\\lambda) = (x_1^2+x_2^2+x_3^2)-\\mu_1(x_1+x_2+x_3-3)\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\nabla_x L(x,\\mu,\\lambda) = (2x_1-\\mu_1,2x_2-\\mu_1,2x_3-\\mu_1)\n",
    "$$\n",
    "\n",
    "and if $\\nabla_x L([1,1,1],\\mu,\\lambda)=0$, then \n",
    "\n",
    "$$\n",
    "2-\\mu_1=0 $$\n",
    "which holds when $$\n",
    "\\mu_1=2.\n",
    "$$\n",
    "\n",
    "In addition to this, we can see that $x^*_1+x^*_2+x^*_3-3= 0$. Thus, the completementarity rule holds even though $\\mu_1\\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT conditions for a solution that is not a local optimum. Let us have $x^*=(0,1,1)$.\n",
    "\n",
    "We can easily see that in this case, the conditions are \n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{array}{c}\n",
    "-\\mu_1 = 0\\\\\n",
    "2-\\mu_1=0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Clearly, there does not exist a $\\mu_1\\in \\mathbb R$ such that these equalities would hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT conditions for another solution that is not a local optimum. Let us have $x^*=(2,2,2)$.\n",
    "\n",
    "We can easily see that in this case, the conditions are\n",
    "\n",
    "$$\n",
    "4-\\mu_1 = 0\n",
    "$$\n",
    "\n",
    "Now, $\\mu_1=4$ satisfies this equation. However, now\n",
    "\n",
    "$$\n",
    "\\mu_1(x^*_1+x^*_2+x^*_3-3)=4(6-3) = 12 \\neq 0.\n",
    "$$\n",
    "\n",
    "Thus, the completementarity rule fails and the KKT conditions are not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometric interpretation of the KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stationary rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stationary rule can be written as: There exist $\\mu,\\lambda'$ so that\n",
    "\n",
    "$$\n",
    "-\\nabla f(x) = -\\sum_{j=1}^K\\mu_j\\nabla g_j(x) + \\sum_{k=1}^K\\lambda'_k\\nabla h_k(x).\n",
    "$$\n",
    "\n",
    "Notice that we have slightly different $\\lambda'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, remember that the $-\\nabla v(x)$ gives us the direction of reduction for a function $v$.\n",
    "\n",
    "Thus, the above equation means that the direction of reduction of the function $-\\nabla f(x)$ is countered by the direction of the reduction of the inequality constraints $-\\nabla g_j(x)$ and the directions of either growth (or reduction, since $\\lambda'$ can be negative) of the equality constraints $\\nabla h_k(x)$.\n",
    "\n",
    "**This means that the function cannot get reduced without reducing the inequality constraints (making the solution infeasible, if already at the bound), or increasing or decreasing the equality constraints (making, thus, the solution again infeasible).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With just one inequality constraint this means that the negative gradients of $f$ and $g$ must point to the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![alt text](images/KKT_inequality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With equality constraints this means that the negative gradient of the objective function and the gradient of the equality constraint must either point to the same or opposite directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/KKT_equality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complementarity conditions\n",
    "Another way of expressing complementarity condition\n",
    "\n",
    "$$\n",
    "\\mu_jg_j(x) = 0 \\text{ for all } j=1,\\ldots,J\n",
    "$$\n",
    "\n",
    "is to say that both $\\mu_j$ and $g_j(x)$ cannot be positive at the same time. Especially, if $\\mu_j>0$, then $g_j(x)=0$.\n",
    "\n",
    "**This means that if we want to use the gradient of a constraint for countering the reduction of the function, then the constraint must be at the boundary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Quadratic Programming (SQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea is to generate a sequence of quadratic optimization problems whose solutions approach the solution of the original problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us consider problem\n",
    "\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t.Â }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "\n",
    "where the the objective function and the equality constraints are twice differentiable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inequality constraints can be handled e.g. by using active set methods. See e.g. https://optimization.mccormick.northwestern.edu/index.php/Sequential_quadratic_programming\n",
    "\n",
    "**Note that constraints can be nonlinear.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we know that the optimal solution of this problem satisfies the KKT conditions, we know that\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "\\nabla_xL(x,\\lambda)=\\nabla_x f(x) + \\lambda\\nabla_x h(x) = 0\\\\\n",
    "h(x) = 0\n",
    "\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us assume that we have a current estimation for the solution of the equality constraints $(x^k,\\lambda^k)$, then according to the Newton's method for root finding (see e.g., https://en.wikipedia.org/wiki/Newton%27s_method#Generalizations ), we have another solution $(x^k,\\lambda^k)^T+(p,v)^T$ of the problem by solving system of equations\n",
    "\n",
    "$$\n",
    "\\nabla_{x,\\lambda} S(x^k,\\lambda^k)\\left[\\begin{align}p^T\\\\v^T\\end{align}\\right] = -S(x^k,\\lambda^k),\n",
    "$$\n",
    "\n",
    "where $S(x^k,\\lambda^k)=\\left[\n",
    "\\begin{array}{c}\n",
    "\\nabla_{x}L(x^k,\\lambda^k)\\\\\n",
    "h(x^k)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be written as\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\nabla^2_{xx}L(x^k,\\lambda^k)&\\nabla_x h(x^k)\\\\\n",
    "\\nabla_x h(x^k)^T & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{c}p^T\\\\v^T\\end{array}\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "-\\nabla_x L(x^k,\\lambda^k)\\\\\n",
    "-h(x^k)^T\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, the above is just the solution of the quadratic problem with equality constraints\n",
    "$$\n",
    "\\min_p \\frac12 p^T\\nabla^2_{xx}L(x^k,\\lambda^k)p+\\nabla_xL(x^k,\\lambda^k)^Tp\\\\\n",
    "\\text{s.t. }h_j(x^k) + \\nabla h_j(x^k)^Tp = 0. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuitive interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are approximating the Lagrange function quadratically around the current solution and the constraints are approximated linearly. SQP methods are also referred to as *projected Lagrangian methods* \n",
    "* compare with projected gradient method from lecture 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Another view point to building the approximation**: Assume that we have a current solution candidate $(x^k,\\lambda^k)$. Using Taylor's series for the constraints ($d = x^*-x^k$) and including only the first order term we get\n",
    "\n",
    "$h_i(x^*)=h_i(x^k+d)\\approx h_i(x^k) + \\nabla h_i(x^k)^Td$\n",
    "\n",
    "Since, $h_i(x^*)=0$ for all $i$ we have\n",
    "\n",
    "$\\nabla h(x^k)d = -h(x^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For approximating the Lagrangian function, we use up to second order terms and get\n",
    "\n",
    "$L(x^k+d,\\lambda^*) \\approx L(x^k,\\lambda^*) + d^T\\nabla_x L(x^k,\\lambda^*) + \\frac{1}{2}d^T\\nabla_{xx}^2L(x^k,\\lambda^*)d$\n",
    "\n",
    "When combining both approximations, we get a quadratic subproblem\n",
    "\n",
    "$$\n",
    "\\underset{d}{\\min}d^T\\nabla_x L(x^k,\\lambda^k) + \\frac{1}{2}d^T \\nabla_{xx}^2L(x^k,\\lambda^k)d\\\\\n",
    "\\text{s.t. } \\nabla h(x^k)d = -h(x^k)\n",
    "$$\n",
    "\n",
    "It can be shown (under some assumptions) that solutions of the quadratic subproblems approach $x^*$ and Lagrange multipliers approach $\\lambda^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that we can either use the exact Hessian of the Lagrange function (requires second derivatives) or some approximation of it (compare Newton's method vs. quasi-Newton ideas). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define an optimization problem, where\n",
    "* $f(x) = \\|x\\|^2 = \\sum_{i=1}^n x_i^2$\n",
    "* $h(x) = \\sum_{i=1}^nx_i=n$\n",
    "\n",
    "What is $x^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_constrained(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-len(x)]\n",
    "#    return sum([i**2 for i in x]),[],[sum(x)-len(x),x[0]**2+x[1]-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_constrained([1,0,1]))\n",
    "print(f_constrained([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f,x,l,k):\n",
    "    #Define the lagrangian for given f and Lagrangian multiplier vector l\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2])*np.matrix(l).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y:\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,l):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "    np.matrix(diff_L(f,x,l,1)),\\\n",
    "    np.matrix(grad_h(f,x)).transpose()),axis=1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "    np.matrix(grad_h(f,x)),\\\n",
    "    np.matrix(np.zeros((len(f(x)[2]),len(f(x)[2]))))),axis=1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "    -1*np.matrix(diff_L(f,x,l,0)).transpose(),\n",
    "    -np.matrix(f(x)[2]).transpose()),axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row),axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side,right_hand_side)\n",
    "    return temp[:len(x)],temp[len(x):] # update for both x and l\n",
    "    \n",
    "    \n",
    "\n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    l = np.ones(len(f(x)[2])) # initialize Lagrange multiplier vector l as a vector of 1s\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        print(x)\n",
    "        f_old = f_new\n",
    "        (p,v) = solve_QP(f,x,l) # obtain updates for x and l by solving the quadratic subproblem\n",
    "        x = x+np.array(p.transpose())[0] # update the solution x \n",
    "        l = l+v # update the Lagrange multipliers l\n",
    "        f_new = f(x)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [2000.0,1000.0,3000.0,5000.0,6000.0]\n",
    "SQP(f_constrained,x0,0.0001)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
