{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4: Steepest descent and Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us define the same function as on the previous lecture for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient-based optimization\n",
    "\n",
    "If gradients are available, they should always be used --> improve convergence\n",
    "\n",
    "Gradient-based methods usually converge to a local minimum which is nearest to the starting point (**Why?**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If analytical formulas of the gradients are not available, one way is to numerically approximate them by using finite differences ($‚Ñé>0$)\n",
    "* forward difference: $\\frac{f(x+h)-f(x)}{h}=\\nabla f(x)+O(h)$\n",
    "* central difference: $\\frac{f(x+h)-f(x-h)}{2h}=\\nabla f(x)+O(h^2)$\n",
    "* central difference is more accurate but requires twice as many function evaluations (in $ùë•+‚Ñé$ and $ùë•‚àí‚Ñé$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation in Python\n",
    "\n",
    "An alternative way is to use automatic differentiation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Automatic_differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Import automatic differentiation package for Python\n",
    "\n",
    "Needs to be installed typing\n",
    "```\n",
    "pip install ad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can ask for gradient and hessian using the <pre>ad.gh</pre> function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " Let us use automatic differentiation for the function <it>f</it>  that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "grad_f, hess_f = ad.gh(f_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At the point (1,2) gradient is \", grad_f([1,2]), \" and hessian is \",hess_f([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let us visualize the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "def visualize_gradient(f,point,x_lim,y_lim):\n",
    "    grad_point = np.array(ad.gh(f)[0](point)) # computes the gradient\n",
    "    grad_point = 10.0*grad_point/np.linalg.norm(grad_point) # scale the gradient vector so that it is easier to see\n",
    "    X,Y,Z = point[0],point[1],f(point) # point that we are interested in and the function value in that point \n",
    "    U,V,W = grad_point[0],grad_point[1],0 # gradient vector, last component is zero since the gradient has 2 components\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d') \n",
    "    x = np.arange(x_lim[0],x_lim[1],0.1) # divide interval for x\n",
    "    y = np.arange(y_lim[0],y_lim[1],0.1) # divide interval for y\n",
    "    X2,Y2 = meshgrid(x, y) # make a grid for x and y\n",
    "    Z2 = [f([x,y]) for (x,y) in zip (X2,Y2)] # evaluation of the function on the grid\n",
    "    Z2 = np.asarray(Z2)\n",
    "    surf = ax.plot_surface(X2, Y2, Z2,alpha=0.5) # surface plot for the function values\n",
    "    ax.quiver(X,Y,Z,U,V,W,color='red',linewidth=2.5) # plots a 2d arrow in a 3d figure\n",
    "    return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_gradient(f_simple,[1,-2],[0,10],[-10,0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the lambda function we can easily visualize gradients of various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "visualize_gradient(lambda x:3*x[0]+x[1],[1,1],[0,2],[0,2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Base algorithm for the steepest descent and Newton's algorithms\n",
    "**Input:** function $f$ to be optimized, starting point $x_0$, step length rule $alpha$, stopping rule $stop$  \n",
    "**Output:** A solution $x^*$ that is close to a locally optimal solution\n",
    "```\n",
    "set f_old as a big number and f_new as f(x0)\n",
    "while a stopping criterion has not been met:\n",
    "    f_old = f_new\n",
    "    determine search direction d_h according to the method\n",
    "    determine the step length alpha\n",
    "    set x = x + alpha *d_h\n",
    "    f_new = f(x)\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to determine search direction distinguishes steepest descent algorithm and the Newton algorithm. Different stopping rules and step sizes can be mixed and matched with both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steepest Descent algorithm for unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as *gradient descent* or *gradient method*\n",
    "\n",
    "In the steepest descent algorithm, the search direction is determined by the negative of the gradient $-\\nabla f(x)$ which (locally) gives the direction of the steepest descent of $f$ at point $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple stopping rule, where we stop when the change is not bigger than precision and we have a fixed step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "    #while np.linalg.norm(ad.gh(f)[0](x))>precision: # an alternative stopping rule\n",
    "        f_old = f_new # store value at the current point\n",
    "        d = -np.array(ad.gh(f)[0](x)) # search direction\n",
    "        x = x+d*step # take a step\n",
    "        f_new = f(x) # compute function value at the new point\n",
    "        steps.append(list(x)) # save step\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solve the problem using the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "step_size = 0.1\n",
    "precision = 0.01\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps of solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    #print(myvec)\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other stopping rules\n",
    "* change in function values: $|f_{new}-f_{old}| \\leq $precision \n",
    "\n",
    "This one we already used. Others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* norm of the gradient: $||\\nabla f(x_{new})|| \\leq $precision\n",
    "* change in variable values: $||x_{new}-x_{old}|| \\leq $precision\n",
    "* maximum number of iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Another example\n",
    "def f_simple2(x):\n",
    "    return (x[0] - 2.0)**4 + (x[0] - 2.0*x[1])**2\n",
    "# Optimal solution is (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "step_size = .3\n",
    "precision = 0.001\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple2,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))\n",
    "#print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare steepest descent with optimized step size:\n",
    "![](images/steepest_descent.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on setting the search direction as $d^h=-[H(x^h)]^{-1}\\nabla f(x^h).\\hspace{2.0cm}$   --> **uses second derivatives!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In one-dimensional case by the Taylor series:\n",
    "$$f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x+\\frac12f''(x)\\Delta x^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to find $x$ such that $f(x)$ is at minimum and, thus, we seek to solve the equation that sets the derivative of this expression with respect to $\\Delta x$ equal to zero:\n",
    "\n",
    "$$ 0 = \\frac{d}{d\\Delta x} \\left(f(x^h)+f'(x^h)\\Delta x+\\frac 1 2 f''(x^h) \\Delta x^2\\right) = f'(x^h)+f'' (x^h) \\Delta x.$$\n",
    "\n",
    "The solution of the above equation is $\\Delta x=-f'(x^h)/f''(x^h)$. Thus, the best approximation of $x^{h+1}$ as the minimum is $x^h-f''(x^h)^{-1}f'(x^h)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def newton(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new # store value of f at the current point\n",
    "        H_inv = np.linalg.inv(np.matrix(ad.gh(f)[1](x))) # compute the inverse of the Hessian\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose() # compute search direction\n",
    "        #Change the type from np.matrix to np.array so that we can use it in our function\n",
    "        x = np.array(x+d*step)[0] # take the step\n",
    "        f_new = f(x) # compute function value at the new point\n",
    "        steps.append(list(x)) # store step\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "step_size = 0.1\n",
    "precision = 0.001\n",
    "(x_value,f_value,steps) = newton(f_simple,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "step_size = 1.0\n",
    "precision = 0.001\n",
    "(x_value,f_value,steps) = newton(f_simple2,start,step_size,precision)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compare Newton's method with step size = 1.0:\n",
    "![](images/newton.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "Steepest descent\n",
    "* Descent direction (does the search direction guarantee decrease in $f$): yes\n",
    "* Global convergence (converges from any starting point): yes\n",
    "* Local convergence: zig-zag near optimum\n",
    "* Computational bottle neck: step length\n",
    "* Memory consumption: $ùëÇ(ùëõ)$\n",
    "\n",
    "Newton‚Äôs method\n",
    "* Descent direction: only if $ùêª(ùë•)^{‚àí1}$ positive definite\n",
    "* Global convergence: no\n",
    "* Local convergence: yes, good if ùëì quadratic\n",
    "* Computational bottle neck: $ùêª(ùë• )^{‚àí1}$ --> can be relaxed by using approximations of $H(x)^{-1}$ --> quasi-Newton\n",
    "* Memory consumption: $ùëÇ(ùëõ^2)$ --> can be reduced e.g. by using conjugate gradient methods"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
