{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Direct search methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization problem to be studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start studying functions of multiple variables by studying unconstrained optimization problems\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.}\\quad &x\\in \\mathbb R^n\n",
    "\\end{align}  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As an example, we study the optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & (x_1-10)^2+(x_2+5)^2+x_1^2\\\\\n",
    "\\text{s.t.}\\quad &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}  \n",
    "$$\n",
    "This problem is unconstrained, because there are no constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Optimality conditions\n",
    "(*Necessary condition*) Let $f$ be twice differentiable at $x^*\\in\\mathbb R^n$. If $x^*$ is a local minimizer, then $\\nabla f(x^*)=0$ and the Hessian matrix $H(x^*)$ is positively semidefinite.\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Hessian_matrix\">What is a Hessian matrix?</a>\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Definiteness_of_a_matrix\">What means definiteness for matrices?</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(*Sufficient condition*) Let $f$ be twice continuously differentiable at $x^*\\in\\mathbb R^n$. If $\\nabla f(x^*)=0$ and $H(x^*)$ is positively definite, then $x^*$ is a strict local minimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When $\\nabla f(x^*)=0$, then $x^*$ is said to be a **critical point**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider our example problem:\n",
    "\n",
    "$\\nabla \\left [(x_1-10)^2+(x_2+5)^2+x_1^2\\right] = (2x_1-20+2x_1,2x_2+10)$\n",
    "\n",
    "$=(4x_1-20,2x_2+10)=0 \\iff x=(5,-5)$\n",
    "\n",
    "By the sufficient condition, $(5,-5)$ is a strict local minimizer. (...*verify that $H(x)$ is positive definite*) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we need to define a function in Python that evaluates that function. That is, we define a two-variable function \n",
    "$$f:(x_1,x_2)\\to (x_1-10)^2+(x_2+5)^2+x_1^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At point (3,-8) the value of the function is \",f_simple([3,-8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also plot that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_2d_function(lb1,lb2,ub1,ub2,f):\n",
    "    x = np.arange(lb1,ub1,0.1) # divides the interval [lb1,ub1] into set of equally divided points \n",
    "    y = np.arange(lb2,ub2,0.1) # divides the interval [lb2,ub2] into set of equally divided points\n",
    "    X,Y = meshgrid(x, y) # grid of points (x_i, y_j)\n",
    "    Z = [f([x,y]) for (x,y) in zip (X,Y)] # evaluation of the function f on the grid\n",
    "    Z = np.asarray(Z)\n",
    "    fig = plt.figure() # initializes figure\n",
    "    ax = fig.gca(projection='3d') # 3D\n",
    "    surf = ax.plot_surface(X, Y, Z) # surface plot of f\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_function(3,-7,7,-3,f_simple).show() # plot f when 3<x1<7 and -7<x2<-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Direct search methods\n",
    "\n",
    "Direct search methods (also called pattern search methods) just rely on the function values to find a (local) optimum. Direct search methods consist of a set of  \n",
    "1. **exploratory moves** that acquire information about the function $f$ in the neighbourhood of the current solution $x$, and\n",
    "2. **pattern moves** that attempt to speed up the search using the information acquired in the exploratory moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SImplest method: Coordinate descent\n",
    "Optimize function towards each coordinate direction separately, *does not utilize pattern moves*.\n",
    "\n",
    "![alt text](images/coordinate_descent.png \"Coordinate descent\")\n",
    "\n",
    "$f(x)=2x_1^2+2x_1x_2+x_2^2+x_1-x_2$\n",
    "\n",
    "* Slow convergence, especially when the level curves of the function are not along any of the coordinate axes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hooke&Jeeves (1961)\n",
    "\n",
    "**input:** a minimum step length $L>0$, initial step length $\\epsilon_0$, constant $0<\\delta<1$ for reducing the step length, exploratory step multiplier $\\gamma>1$, and starting solution $x_0$  \n",
    "**output:** an approximation of a local optimum (no guarantees of quality in general cases)  \n",
    "```\n",
    "set epsilon as the initial step length epsilon0\n",
    "set x as the starting solution\n",
    "while epsilon is greater than L:\n",
    "    for each coordinate direction i:\n",
    "        find the smallest of function values by incrementing and reducing the \n",
    "        variable value in that coordinate by epsilon, let this value be xi*\n",
    "    if x is the same as (x1*,...,xn*):\n",
    "        reduce epsilon to delta*epsilon\n",
    "    else:\n",
    "        if the value of f at (x1*,...,xn*) is smaller than at x+gamma*((x1*,...,xn*)-x):\n",
    "            set x as (x1*,...xn*)\n",
    "        else:\n",
    "            set x as x+gamma*((x1*,...,xn*)-x)\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the exploratory step of Hooke&Jeeves is performed by incrementing and reducing the variable to each coordinate direction and the pattern move is just a multiplication of the exploratory move.\n",
    "\n",
    "![alt text](images/hooke&Jeeves.svg \"Hooke&Jeeves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import copy #Copying vectors\n",
    "import numpy as np #Import vector calculus and much more!\n",
    "def hookejeeves(L,epsilon0,delta,gamma,x0,f):\n",
    "    #Set up the initial values\n",
    "    epsilon = epsilon0\n",
    "    x = np.array(x0)\n",
    "    #Loop while step length epsilon is greater than L:\n",
    "    while epsilon>L:\n",
    "        #our exploratory move is initially [0,..,0]\n",
    "        xtest = np.zeros(len(x))\n",
    "        for coordinate in range(len(x)):\n",
    "            #First points to be explored are the all x, to be changed\n",
    "            exp_points = [copy.copy(x) for _ in range(3)] #points to be explored\n",
    "            #Change exp_points[0] and exp_points[1] to reflect\n",
    "            #moving along the coordinate\n",
    "            exp_points[0][coordinate]-=epsilon\n",
    "            exp_points[1][coordinate]+=epsilon\n",
    "            #Assign the function values given by exp_points to a list\n",
    "            f_exp_points = [f(exp_point) for exp_point in exp_points]\n",
    "            #pick the smallest one of them\n",
    "            min_value = min(f_exp_points)\n",
    "            #The exploratory move to the coordinate direction is given by the\n",
    "            #move giving the smallest value of f\n",
    "            xtest[coordinate] = exp_points[f_exp_points.index(min_value)][coordinate] #The coordinate value is the one where the minimum is attained\n",
    "        #If no move at all, then reduce the exploratory move step size\n",
    "        if all(xtest==x):\n",
    "            epsilon = delta*epsilon\n",
    "        else:\n",
    "            #if exploratory move is better than pattern move\n",
    "            if f(xtest)<f(x+gamma*(xtest-x)):\n",
    "                #...set x as the exploratory move\n",
    "                x = xtest\n",
    "            else:\n",
    "                #Otherwise we take the pattern move\n",
    "                x = x+gamma*(xtest-x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 1\n",
    "delta = 0.1\n",
    "gamma = 2.0\n",
    "start = [2.0,3.0]\n",
    "#%timeit hookejeeves(L,epsilon0,delta,gamma,x0,f_simple)\n",
    "x = hookejeeves(L,epsilon0,delta,gamma,start,f_simple); print(\"Optimal solution is \" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a function that can plot a set of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro') # plotting only for problems with 2 variables\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we need to save the steps of the algorithm. For that, let us modify the algorithm slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy #Copying vectors\n",
    "import numpy as np #Import vector calculus and much more!\n",
    "def hookejeeves_savesteps(L,epsilon0,delta,gamma,x0,f):\n",
    "    epsilon = epsilon0\n",
    "    x = np.array(x0)\n",
    "    steps = []\n",
    "    while epsilon>L:\n",
    "        xtest = np.zeros(len(x))\n",
    "        # exploratory move\n",
    "        for coordinate in range(len(x)):\n",
    "            exp_points = [copy.copy(x) for _ in range(3)] #points to be explored\n",
    "            exp_points[1][coordinate]-=epsilon\n",
    "            exp_points[2][coordinate]+=epsilon\n",
    "            f_exp_points = [f(exp_point) for exp_point in exp_points]\n",
    "            min_value = min(f_exp_points)\n",
    "            xtest[coordinate] = exp_points[f_exp_points.index(min_value)][coordinate] #The coordinate value is the one where the minimum is attained\n",
    "        if all(xtest==x):\n",
    "            # exploratory move is zero\n",
    "            epsilon = delta*epsilon\n",
    "            print(epsilon)\n",
    "        else:\n",
    "            # check if pattern move helps\n",
    "            if f(xtest)<f(x+gamma*(xtest-x)):\n",
    "                x = xtest\n",
    "            else:\n",
    "                x = x+gamma*(xtest-x)\n",
    "            steps.append(x)\n",
    "            #print(x)\n",
    "    return x,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 =  0.3\n",
    "delta = 0.1\n",
    "gamma = 2\n",
    "start = [2,3]\n",
    "(x,steps) = hookejeeves_savesteps(L,epsilon0,delta,gamma,start,f_simple)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The role of the step length \n",
    "The above implementation uses a fixed step length. It is easier to implement but not as efficient if the step length would be optimized by using a line search. The difference is illustrated in the figures below.\n",
    "\n",
    "Fixed                    |Optimized\n",
    ":-----------------------:|:----------------------------:\n",
    "![](images/hj_fixed.png) | ![](images/hj_optimized.png)\n",
    "\n",
    "**Note: You can implement a version optimizing the step length by line search e.g. when solving future excercises**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Powell's method (1964)\n",
    "\n",
    "Powell's method is similar to Hooke&Jeeves, but the first step in exploratory moves is taken to the direction of the last pattern move. This speeds up the convergence in most cases (**highly depending on how the step size is computed!**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "def powell(L,epsilon0,delta,gamma,x0,f):\n",
    "    epsilon = epsilon0\n",
    "    exp_direction = np.array([0,1])\n",
    "    x = np.array(x0)\n",
    "    while epsilon>L:\n",
    "        exp_direction=epsilon*exp_direction # to take steps of length epsilon\n",
    "        #Comparing among exploratory points to first exploratory direction:\n",
    "        if f(x+exp_direction)<f(x):\n",
    "            exp_step1=exp_direction\n",
    "        elif f(x-exp_direction)<f(x):\n",
    "            exp_step1=-exp_direction\n",
    "        else:\n",
    "            exp_step1 = np.zeros(2)\n",
    "        #The following only works in 2d!!\n",
    "        exp_direction2 = np.array([exp_direction[1],-exp_direction[0]]) # conjugate directions\n",
    "        if f(x+exp_direction2)<f(x):\n",
    "            exp_step2=exp_direction2\n",
    "        elif f(x-exp_direction2)<f(x):\n",
    "            exp_step2=-exp_direction2\n",
    "        else:\n",
    "            exp_step2 = np.zeros(2)\n",
    "        if all(exp_step1+exp_step2==0):\n",
    "            epsilon = delta*epsilon\n",
    "        else:\n",
    "            if f(x+(exp_step1+exp_step2))<f(x+gamma*(exp_step1+exp_step2)):\n",
    "                x = x+(exp_step1+exp_step2)\n",
    "            else:\n",
    "                x = x+gamma*(exp_step1+exp_step2)\n",
    "            exp_direction = epsilon*(exp_step1+exp_step2)/np.linalg.norm(exp_step1+exp_step2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 1.\n",
    "delta = 0.1\n",
    "gamma = 2.0\n",
    "start = [2.,3.]\n",
    "#%timeit powell(L,epsilon0,delta,gamma,x0,f_simple)\n",
    "x = powell(L,epsilon0,delta,gamma,start,f_simple)\n",
    "print(\"Optimal solution is \" + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let us again modify the algorithm slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "def powell_savesteps(L,epsilon0,delta,gamma,x0,f):\n",
    "    epsilon = epsilon0\n",
    "    exp_direction = np.array([0,1])\n",
    "    x = np.array(x0)\n",
    "    steps = []\n",
    "    while epsilon>L:\n",
    "        exp_direction=epsilon*exp_direction\n",
    "        #Comparing among exploratory points to firtst exploratory direction:\n",
    "        if f(x+exp_direction)<f(x):\n",
    "            exp_step1=exp_direction\n",
    "        elif f(x-exp_direction)<f(x):\n",
    "            exp_step1=-exp_direction\n",
    "        else:\n",
    "            exp_step1 = np.zeros(2)\n",
    "        #The following only works in 2d!!\n",
    "        exp_direction2 = np.array([exp_direction[1],-exp_direction[0]])\n",
    "        if f(x+exp_direction2)<f(x):\n",
    "            exp_step2=exp_direction2\n",
    "        elif f(x-exp_direction2)<f(x):\n",
    "            exp_step2=-exp_direction2\n",
    "        else:\n",
    "            exp_step2 = np.zeros(2)\n",
    "        if all(exp_step1+exp_step2==0):\n",
    "            epsilon = delta*epsilon\n",
    "            print(epsilon)\n",
    "        else:\n",
    "            if f(x+(exp_step1+exp_step2))<f(x+gamma*(exp_step1+exp_step2)):\n",
    "                x = x+(exp_step1+exp_step2)\n",
    "            else:\n",
    "                x = x+gamma*(exp_step1+exp_step2)\n",
    "            steps.append(x)\n",
    "            exp_direction = (exp_step1+exp_step2)/np.linalg.norm(exp_step1+exp_step2)\n",
    "            #print(x)\n",
    "    return x,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 0.3\n",
    "delta = 0.1\n",
    "gamma = 2\n",
    "start = [2,3]\n",
    "(x,steps) = powell_savesteps(L,epsilon0,delta,gamma,start,f_simple)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another example\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & (x_1-2)^4+(x_1 - 2x_2)^2\\\\\n",
    "\\text{s.t.}\\quad &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}  \n",
    "$$\n",
    "Optimal solution clearly is $x^*=(2,1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f_simple2(x):\n",
    "    return (x[0] - 2.0)**4 + (x[0] - 2.0*x[1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_function(-2,-6,6,4,f_simple2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hooke & Jeeves\n",
    "L = 0.0001\n",
    "epsilon0 = .8\n",
    "delta = 0.1\n",
    "gamma = 2\n",
    "start = [-1.,3.]\n",
    "(x,steps) = hookejeeves_savesteps(L,epsilon0,delta,gamma,start,f_simple2)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Powell\n",
    "L = 0.0001\n",
    "epsilon0 = .8\n",
    "delta = 0.1\n",
    "gamma = 2.\n",
    "start = [-1.,3.]\n",
    "(x,steps) = powell_savesteps(L,epsilon0,delta,gamma,start,f_simple2)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just another example\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\quad & x_1^2+2x_1x_2+x_2^2+x_1-x_2\\\\\n",
    "\\text{s.t.}\\quad &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}  \n",
    "$$\n",
    "Optimal solution is $x^*=(-1,1.5)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def f_simple3(x):\n",
    "    return 2.0*x[0]**2 + 2.0*x[0]*x[1] + x[1]**2 + x[0] - x[1]\n",
    "# opt = (-1,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_function(-4,-4,2,6,f_simple3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_simple3([-1.,1.4]))\n",
    "print(f_simple3([-.9,1.5]))\n",
    "print(f_simple3([-1.,1.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hooke & Jeeves\n",
    "L = 0.001\n",
    "epsilon0 = 1.\n",
    "delta = 0.1\n",
    "gamma = 2\n",
    "start = [-2.0,3.0]\n",
    "(x,steps) = hookejeeves_savesteps(L,epsilon0,delta,gamma,start,f_simple3)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "L = 0.001\n",
    "epsilon0 = 1.\n",
    "delta = 0.1\n",
    "gamma = 2.0\n",
    "start = [-2.,3.]\n",
    "(x,steps) = powell_savesteps(L,epsilon0,delta,gamma,start,f_simple3)\n",
    "print(x)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nelder Mead method (by J. Nelder & R. Mead in 1965)\n",
    "\n",
    "A direct search method based on a simplex, i.e. $n+1$ points in $n$-dimensional space. The points for a simplex are iteratively updated by replacing the worst point based on the objective function value. Further information and illustration can be found e.g. in <a href=\"https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method\">Nelder-Mead</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
