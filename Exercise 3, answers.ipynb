{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises 3, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x): # function to be optimized\n",
    "    return (x[0]**2.0 + x[1]**2.0 + x[0] + 2.0 * x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\nabla f(x) = (2x_1+1,2x_2+2)=0$ if and only if $x_1=-0.5$ and $x_2 = -1.0$. Thus $x^*=(-0.5,-1.0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task 1\n",
    "* max 3 points, 3 points if one gets correct solution and line search is properly done, reductions if something is not done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def golden_section_line_search(a,b,f,L): # same as in the lectures\n",
    "    x = a\n",
    "    y = b\n",
    "    while y-x>2*L:\n",
    "        if f(x+(math.sqrt(5.0)-1)/2.0*(y-x))<f(y-(math.sqrt(5.0)-1)/2.0*(y-x)):\n",
    "            x = y-(math.sqrt(5.0)-1)/2.0*(y-x)\n",
    "        else:\n",
    "            y = x+(math.sqrt(5.0)-1)/2.0*(y-x)\n",
    "    return (x+y)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent_withgolden(f,start,search_interval_length,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    d = float('Inf')\n",
    "    while np.linalg.norm(d)>precision:\n",
    "        f_old = f_new\n",
    "        d = -np.array(ad.gh(f)[0](x)) # search direction of the steepest descent\n",
    "        # step length optimization with golden section\n",
    "        step = golden_section_line_search(0,\n",
    "                                          search_interval_length/np.linalg.norm(d), # normalization, can be done earlier as well\n",
    "                                          lambda t: f(x+t*d), # function with respect to step length t\n",
    "                                          precision)\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start = [-5,10]\n",
    "precision = 0.0001\n",
    "(x,f_new,steps1) = steepest_descent_withgolden(objective_function,\n",
    "                                               start,10,precision)\n",
    "print (x)\n",
    "print(len(steps1))\n",
    "print(steps1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task 2\n",
    "* max 2 points, 1 point for plot, 1 point if there is comparison/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent(f,start,step,precision): # from the lectures\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    d = float('Inf')\n",
    "#    while abs(f_old-f_new)>precision and len(steps)<10:\n",
    "    while np.linalg.norm(d)>precision and len(steps)<20:\n",
    "        f_old = f_new\n",
    "        d = -np.array(ad.gh(f)[0](x))\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "step = 0.2\n",
    "(x,f_new,steps2) = steepest_descent(objective_function,start,step,precision)\n",
    "print(x)\n",
    "print(len(steps2))\n",
    "print(steps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps2(steps1,start1,steps2,start2):\n",
    "    myvec1 = np.array([start1]+steps1).transpose()\n",
    "    myvec2 = np.array([start2]+steps2).transpose()\n",
    "    plt.plot(myvec2[0,],myvec2[1,],'bo')    \n",
    "    plt.plot(myvec1[0,],myvec1[1,],'rx')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps2(steps1,start,steps2,start).show() # optimized blue, fixed red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Remarks**\n",
    "* With fixed step size the steps are not actually of equal length since the step size is multiplied with the gradient whose length varies\n",
    "* The performance of steepest descent with optimized step size also depends on what is set as the maximum step length\n",
    "  * too long --> golden section search uses a high nuber of function evaluations\n",
    "  * too short --> takes shorter steps where improvement could still be made with longer ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task 3\n",
    "* max 3 points, 3 points if one gets correct result, reductions is there are flaws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_k=\\nabla f(x_k+s_k)-\\nabla f(x_k),$$\n",
    "\n",
    "$$s_k=x_{k+1} -x_k=(x_k+s_k)-x_k$$\n",
    "\n",
    "$$H_{k+1}=H_{k}-\\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}+\\frac{s_k s_k^T}{y_k^{T} s_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "import numpy as np\n",
    "def update_Hinv(H_inv_old,x_old,x_new,f): # subroutine to update inverse of the Hessian\n",
    "    y = np.matrix(ad.gh(f)[0](x_new)-ad.gh(f)[0](x_old)).transpose() # compute y_k\n",
    "    second_term = H_inv_old*y*y.transpose()*H_inv_old.transpose()/(y.transpose()*H_inv_old*y)\n",
    "    s = np.matrix(x_new-x_old).transpose() # compute s_k\n",
    "    third_term = s*s.transpose()/(y.transpose()*s)\n",
    "    H_inv_new = H_inv_old-second_term+third_term\n",
    "    return H_inv_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def quasi_newton_DFP(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x_new = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x_new)\n",
    "    # Use identity matrix as the first approximation (is positive definite)\n",
    "    H_inv = np.eye(len(start))\n",
    "    d = float('Inf')\n",
    "#    while abs(f_old-f_new)>precision and len(steps)<10:\n",
    "    while np.linalg.norm(d)>precision and len(steps)<20:\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x_new)).transpose())).transpose()\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        #Change the type from np.matrix to np.array so that we can use it in our function\n",
    "        x_new = np.array(x_new+d*step)[0]\n",
    "        f_new = f(x_new)\n",
    "        steps.append(list(x_new))\n",
    "        H_inv = update_Hinv(H_inv,x_old,x_new,f)\n",
    "    return x_new,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "start = [-3,2]\n",
    "(x,f_new,steps)=quasi_newton_DFP(objective_function,start,0.5,precision)\n",
    "print(x)\n",
    "print(len(steps))\n",
    "print (steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'bo')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
