{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6: Indirect methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some remarks\n",
    "* How to deal with problems where the objective function should be maximized, i.e. $\\max f(x)$?\n",
    "  * We can use the same methods if we instead minimize the negative of $f$, i.e. $\\min -f(x)$\n",
    "  * The optimal solution $x^*$ is the same for both the problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f_max(x):\n",
    "    return -(x-3.0)**2 + 10.0\n",
    "\n",
    "# clearly x* = 3.0 is the global maximum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-5.0, 12.0, 0.3)\n",
    "plt.plot(x, f_max(x), 'bo')\n",
    "plt.show()\n",
    "#print(f1(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# multiply f_max with -1.0\n",
    "def g(x):\n",
    "    return -f_max(x)\n",
    "\n",
    "plt.plot(x, g(x), 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize_scalar(g,method='brent')\n",
    "print(res)\n",
    "print(g(res.x))\n",
    "print(f_max(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move to studying constrained optimization problems i.e., the full problem\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&a_i\\leq x_i\\leq b_i\\text{ for all } i=1,\\ldots,n\\\\\n",
    "&x\\in \\mathbb R^n,\n",
    "\\end{align}\n",
    "$$\n",
    "where for all $i=1,\\ldots,n$ it holds that $a_i,b_i\\in \\mathbb R$ or they may also be $-\\infty$ or $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## On optimal solutions for constrained problems\n",
    "* Two types of constraints: equality and inequality constraints\n",
    "* Inequality constraint $g_i(x)\\geq0$ is said to be *active* at point $x$ if $g_i(x)=0$\n",
    "* Linear constraints are much easier to consider --> their gradients are constant\n",
    "* Nonlinear constraints trickier --> gradient changes for different values of decision variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "No constraints\n",
    "![](images/unconstrained.png)\n",
    "<span style=\"font-size: 10pt;\">*Adopted from Prof. L.T. Biegler (Carnegie Mellon University)*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inequality constraints\n",
    "![](images/inequality_constraints.jpg)\n",
    "<span style=\"font-size: 10pt;\">*Adopted from Prof. L.T. Biegler (Carnegie Mellon University)*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both inequality and equality constraints\n",
    "![](images/constraints.jpg)\n",
    "<span style=\"font-size: 10pt;\">*Adopted from Prof. L.T. Biegler (Carnegie Mellon University)*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transforming the constraints\n",
    "Type of inequality:\n",
    "$$\n",
    "g_i(x)\\geq0 \\iff -g_i(x)\\leq0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Inequality to equality:\n",
    "$$\n",
    "g_i(x)\\leq0 \\iff g_i(x)+y_i^2=0\n",
    "$$\n",
    "* $y_i$ is a *slack variable*; constraint is active if $y_i=0$\n",
    "* By adding $y_i^2$ no need to add $y_i\\geq0$\n",
    "* If $g$ is linear, linearity can be preserved by $g_i(x)+y_i=0, y_i\\geq0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Equality to inequality:\n",
    "$$\n",
    "h_i(x)=0 \\iff h_i(x)\\geq0 \\text{ and } -h_i(x) \\geq0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example problem\n",
    "For example, we can have an optimization problem\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &x_1^2+x_2^2\\\\\n",
    "\\text{s.t.} \\quad & x_1+x_2-1\\geq 0\\\\\n",
    "&-1\\leq x_1\\leq 1, x_2\\leq 3.\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to optimize that problem, we can define the following python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f_constrained(x):\n",
    "    return np.linalg.norm(x)**2,[x[0]+x[1]-1],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained([1,0])\n",
    "print(\"Value of f is \"+str(f_val))\n",
    "if len(ieq)>0:\n",
    "    print(\"The values of inequality constraints are:\")\n",
    "    for ieq_j in ieq:\n",
    "        print(str(ieq_j)+\", \")\n",
    "if len(eq)>0:\n",
    "    print(\"The values of the equality constraints are:\")\n",
    "    for eq_k in eq:\n",
    "        print(str(eq_k)+\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is this solution feasible?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print(\"Solution is feasible\")\n",
    "else:\n",
    "    print(\"Solution is infeasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Indirect and direct methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two categories of methods for constrained optimization: Indirect and direct methods. The main difference is that\n",
    "1. Indirect methods convert the constrained optimization problem into a single or a sequence of unconstrained optimization problems, that are then solved. Often, the intermediate solutions do not need to be feasible, but the sequence of solutions converges to a solution that is optimal for the original problem (and, thus, feasible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Direct methods deal with the constrained optimization problem directly. In this case, all the intermediate solutions are feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Indirect methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Penalty function methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA:** Include constraints into the objective function with the help of penalty functions that penalize constraint violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let, $\\alpha(x):\\mathbb R^n\\to\\mathbb R$ be a function so that \n",
    "* $\\alpha(x)= 0$, for all feasible $x$\n",
    "* $\\alpha(x)>0$, for all infeasible $x$.\n",
    "\n",
    "Define a set of optimization problems (depending on parameter $r$)\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\qquad &f(x)+r\\alpha(x)\\\\\n",
    "\\text{s.t.} \\qquad &x\\in \\mathbb R^n\n",
    "\\end{align}\n",
    "$$\n",
    "for $r>0$. Let $x_r$ be an optimal solution of such problem for a given $r$.\n",
    "\n",
    "In this case, the optimal solutions $x_r$ converge to the optimal solution of the constrained problem, when $r\\to\\infty$, if such a solution exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example, good ideas for penalty functions are\n",
    "* $h_k(x)^2$ for equality constraints,\n",
    "* $\\left(\\min\\{0,g_j(x)\\}\\right)^2$ for inequality constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Illustrative example\n",
    "$$\n",
    "\\min x \\\\\n",
    "\\text{ s.t. } -x + 2 \\leq 0\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "\\alpha(x) = \\{(\\max[0,(-x+2)])^2, 0 \\text{ if }x\\geq2\\}\n",
    "$$\n",
    "Then \n",
    "$$\n",
    "\\alpha(x) = (-x+2)^2, \\text{ if } x<2\n",
    "$$\n",
    "Minimum of $f+r\\alpha(x)$ is at $2-1/2r$\n",
    "![](images/penalty.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def alpha(x,f):\n",
    "    (_,ieq,eq) = f(x)\n",
    "    return sum([min([0,ieq_j])**2 for ieq_j in ieq])+sum([eq_k**2 for eq_k in eq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha([1,0],f_constrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def penalized_function(x,f,r):\n",
    "    return f(x)[0] + r*alpha(x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(penalized_function([-1,0],f_constrained,10000))\n",
    "print(penalized_function([-1,0],f_constrained,100))\n",
    "print(penalized_function([-1,0],f_constrained,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's solve the penalty problem by using Nelder-Mead from scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(lambda x:penalized_function(x,f_constrained,1),\n",
    "               [0,0],method='Nelder-Mead', \n",
    "         options={'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained(res.x)\n",
    "print(\"Value of f is \"+str(f_val))\n",
    "if len(ieq)>0:\n",
    "    print(\"The values of inequality constraints are:\")\n",
    "    for ieq_j in ieq:\n",
    "        print(str(ieq_j)+\", \")\n",
    "if len(eq)>0:\n",
    "    print(\"The values of the equality constraints are:\")\n",
    "    for eq_k in eq:\n",
    "        print(str(eq_k)+\", \")\n",
    "\n",
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print(\"Solution is feasible\")\n",
    "else:\n",
    "    print(\"Solution is infeasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How to set the penalty term $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty term should\n",
    "* be large enough in order for the solutions be close enough to the feasible region, but\n",
    "* not be too large to\n",
    "  * cause numerical problems, or\n",
    "  * cause premature convergence to non-optimal solutions because of relative tolerances.\n",
    "\n",
    "Usually, the penalty term is either\n",
    "* set as big as possible without causing problems (hard to know), or\n",
    "* updated iteratively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note:** \n",
    "\n",
    "* We solved our example problem with a fixed value for the penalty parameter $r$. In order to make the penalty function method work in practice, you have to implement the iterative update for $r$. This you can practice in one of the upcoming exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The starting point for solving the penalty problems can be selected in an efficient way. When you set $r_i$ and solve the corresponding unconstrained penalty problem, you get an optimal solution $x_{r_i}$. Then you update $r_i\\rightarrow r_{i+1}$ and you can use $x_{r_i}$ as a starting point for solving the penalty problem with $r_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Barrier function methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA:** Prevent leaving the feasible region so that the value of the objective is $\\infty$ outside the feasible set.\n",
    "\n",
    "This method is only applicable to problems with inequality constraints and for which the set \n",
    "$$\\{x\\in \\mathbb R^n: g_j(x)>0\\text{ for all }j=1,\\ldots,J\\}$$\n",
    "is non-empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\beta:\\{x\\in \\mathbb R^n: g_j(x)>0\\text{ for all }j=1,\\ldots,J\\}\\to \\mathbb R$ be a function so that $\\beta(x)\\to \\infty$, when $x\\to\\partial\\{x\\in \\mathbb R^n: g_j(x)>0\\text{ for all }j=1,\\ldots,J\\}$, where $\\partial A$ is the boundary of the set $A$. \n",
    "\n",
    "Now, define optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & f(x) + r\\beta(x)\\\\\n",
    "\\text{s.t. } \\qquad & x\\in \\{x\\in \\mathbb R^n: g_j(x)>0\\text{ for all }j=1,\\ldots,J\\}.\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x_r$ be the optimal solution of this problem (which we assume to exist for all $r>0$).\n",
    "\n",
    "In this case, $x_r$ converges to the optimal solution of the problem (if it exists), when $r\\to 0^+$ (i.e., $r$ converges to zero from the right side (= positive numbers))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A good idea for a barrier function is $\\frac1{g_j(x)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(x,f):\n",
    "    _,ieq,_ = f(x)\n",
    "    try:\n",
    "        value=sum([1/max([0,ieq_j]) for ieq_j in ieq])\n",
    "    except ZeroDivisionError:\n",
    "        value = float(\"inf\")\n",
    "    return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def function_with_barrier(x,f,r):\n",
    "    return f(x)[0]+r*beta(x,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's try to find a feasible starting point\n",
    "print(f_constrained([1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(lambda x:function_with_barrier(x,f_constrained,0.0000001),\n",
    "               [1,1],method='Nelder-Mead', options={'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(f_val,ieq,eq) = f_constrained(res.x)\n",
    "print(\"Value of f is \"+str(f_val))\n",
    "if len(ieq)>0:\n",
    "    print(\"The values of inequality constraints are:\")\n",
    "    for ieq_j in ieq:\n",
    "        print(str(ieq_j)+\", \")\n",
    "if len(eq)>0:\n",
    "    print(\"The values of the equality constraints are:\")\n",
    "    for eq_k in eq:\n",
    "        print(str(eq_k)+\", \")\n",
    "if all([ieq_j>=0 for ieq_j in ieq]) and all([eq_k==0 for eq_k in eq]):\n",
    "    print(\"Solution is feasible\")\n",
    "else:\n",
    "    print(\"Solution is infeasible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is 'easy' to see that x* = (0.5,0.5) \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=minimize+x%5E2%2By%5E2+on+x%2By%3E%3D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f_constrained([.5,.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other notes about using penalty and barrier function methods\n",
    "\n",
    "* It is worthwile to consider whether feasibility can be compromized. If the constraints do not have any tolerances, then barrier function method should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Also barrier methods parameter can be set iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Penalty and barrier functions should be chosen so that they are differentiable (thus $x^2$ above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In both methods, the minimum is attained at the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Different penalty and barrier parameters can be used for different constraints, even for same problem."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
