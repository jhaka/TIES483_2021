{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7, direct methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct methods for constrained optimization are also known as *methods of feasible directions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feasible descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let $S\\subset \\mathbb R^n$ ($S\\neq \\emptyset$ closed) and $x^*\\in S$. \n",
    "**Definition:** The set\n",
    "$$ D = \\{d\\in \\mathbb R^n: d\\neq0,x^*+\\alpha d\\in S \\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the cone of feasible directions of $S$ in $x^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** The set \n",
    "$$ F = \\{d\\in \\mathbb R^n: f(x^*+\\alpha d)<f(x^*)\\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the cone of descent directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** The set $F\\cap D$ is called the cone of feasible descent directions.\n",
    "\n",
    "![alt text](images/feasible_descent_directions.svg \"Feasible descent directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**(Obvious) Theorem:** Consider an optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\  f(x)\\\\\n",
    "\\text{s.t. }&\\ x\\in S\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x^*\\in S$. Now if $x^*$ is a local minimizer **then** the set of feasible descent directions $F\\cap D$ is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea for the methods of feasible descent directions\n",
    "\n",
    "1. Assume a feasible solution $x$.\n",
    "2. Find a feasible descent direction $d\\in D\\cap F$.\n",
    "3. Determine the step length to the direction $d$\n",
    "4. Update $x$ accordingly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rosen's projected gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a problem with linear equality constraints\n",
    "\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }Ax=b,\n",
    "$$\n",
    "\n",
    "where $A$ is a $l\\times n$ matrix ($l\\leq n$) and $b$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $x$ be a feasible solution to the above problem.\n",
    "\n",
    "It holds that:\n",
    "\n",
    "$d$ is a feasible direction *if and only if* $Ad=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the gradient $-\\nabla f(x)$ is a feasible descent direction, if \n",
    "\n",
    "$$ A\\nabla f(x)=0.$$\n",
    "\n",
    "This may or may not be true (i.e. the gradient may or may not be a feasible direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, we can project the gradient to the set of feasible descent directions\n",
    "$$ \\{d\\in \\mathbb R^n: Ad=0\\},$$\n",
    "which now is a linear subspace.\n",
    "\n",
    "![alt text](images/subspace.svg \"A linear subspace Ad=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection\n",
    "\n",
    "Let $a\\in \\mathbb R^n$ be a vector and let $L$ be a linear subspace of $\\mathbb R^n$. Now, the following are equivalent\n",
    "* $a^P$ is the projection of $a$ on $L$,\n",
    "* $\\{a^P\\} = \\operatorname{argmin}_{l\\in L}\\|a-l\\|$, and\n",
    "* $a^P\\in A$ and $(a-a^P)^Tl=0$ for all $l\\in L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projected gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of the gradient $\\nabla f(x)$ on the set $\\{d\\in \\mathbb R^n: Ad=0\\}$ is denoted by $\\nabla f(x)^P$ and called the *projected gradient*. \n",
    "\n",
    "Now, given some conditions, the projected gradient gives us a feasible descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/projected_gradient.svg \"A projected gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to compute the projected gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways, but at this course we can use optimization. Basically, the optimization problem that we have to solve is\n",
    "$$\n",
    "\\min \\|\\nabla f(x)-d\\|\\\\\n",
    "\\text{s.t. }Ad=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since it is equivalent to minimize the square of the objective function $\\sum_{i=n}\\nabla_i f(x)^2+d_i^2-2\\nabla_i f(x)d_i$, we can see that the problem is a quadratic problem with equality constraints,\n",
    "$$\n",
    "\\min \\frac12 d^TId-\\nabla f(x)^Td\\\\\n",
    "\\text{s.t. }Ad=0\n",
    "$$\n",
    "which means that we just need to solve the system of equations (see e.g., https://en.wikipedia.org/wiki/Quadratic_programming#Equality_constraints)\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "I&A^T\\\\\n",
    "A&0\n",
    "\\end{array}\n",
    "\\right] \n",
    "\\left[\\begin{align}d\\\\\\lambda\\end{align}\\right]\n",
    "= \\left[ \n",
    "\\begin{array}{c}\n",
    "\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where I is the identity matrix, and $\\lambda$ are the KKT multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function for projecting a vector to a linear space defined by $Ax=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "help(np.linalg.solve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_vector(A,vector):\n",
    "    #convert A into a matrix\n",
    "    A_matrix = np.matrix(A)\n",
    "    #construct the \"first row\" of the matrix [[I,A^T],[A,0]]\n",
    "    left_matrix_first_row = np.concatenate((np.identity(len(vector)),A_matrix.transpose()), axis=1)\n",
    "    #construct the \"second row\" of the matrix\n",
    "    left_matrix_second_row = np.concatenate((A_matrix,np.matrix(np.zeros([len(A),len(A)]))), axis=1)\n",
    "    #combine the whole matrix by combining the rows\n",
    "    left_matrix = np.concatenate((left_matrix_first_row,left_matrix_second_row),axis = 0)\n",
    "    #Solve the system of linear equalities from the previous page\n",
    "    return np.linalg.solve(left_matrix, \\\n",
    "                           np.concatenate((np.matrix(vector).transpose(),\\\n",
    "                                           np.zeros([len(A),1])),axis=0))[:len(vector)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Project gradient such that A*proj_gradient = 0\n",
    "A = [[1,0,0],[0,1,0]]\n",
    "gradient = [1,1,1]\n",
    "project_vector(A,gradient)\n",
    "#print len(A),len(gradient),len(A[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad& x_1^2+x_2^2+x_3^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1+x_2=3\\\\\n",
    "    &x_1+x_3=4.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Let us project a negative gradient from a feasible point $x=(1,2,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the matrix\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{array}{ccc}\n",
    "1& 1 & 0\\\\\n",
    "1& 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "gradient = ad.gh(lambda x:x[0]**2+x[1]**2+x[2]**2)[0]([1,2,3])\n",
    "print(gradient)\n",
    "d = project_vector(A,[-i for i in gradient])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d is a feasible direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matrix(A)*d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d is a descent direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2+x[1]**2+x[2]**2\n",
    "alpha = 0.001\n",
    "print(\"Value of f at [1,2,3] is \"+str(f([1,2,3])))\n",
    "x_mod= np.array([1,2,3])+alpha*np.array(d).transpose()[0]\n",
    "print(x_mod)\n",
    "print(\"Value of f at [1,2,3] +alpha*d is \"+str(f(x_mod)))\n",
    "print(\"Gradient dot product direction (i.e., directional derivative) is \" \\\n",
    "      + str(np.matrix(ad.gh(f)[0]([1,2,3])).dot(np.array(d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally, the algorithm of the projected gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    iters = 0\n",
    "    # this will be filled in during the lecture\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        # store the current function value\n",
    "        f_old = f_new\n",
    "        # compute gradient\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        # project negative gradient\n",
    "        d = project_vector(A,[-i for i in gradient])\n",
    "        # take transpose\n",
    "        d = d.reshape(1,-1)\n",
    "        # take step\n",
    "        x = np.array(x + step*d)[0]\n",
    "        # compute f in new point\n",
    "        f_new = f(x)\n",
    "        # record new step\n",
    "        steps.append(x)\n",
    "        # update iterations counter\n",
    "        iters = iters + 1\n",
    "    return x,f_new,steps,iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:x[0]**2+x[1]**2+x[2]**2\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "start = [1,2,3]\n",
    "(x,f_val,steps,iters) = projected_gradient_method(f,A,start,0.6,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(f(x))\n",
    "print(f([1,2,3]))\n",
    "print(np.matrix(A)*np.matrix(x).transpose())\n",
    "print(iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Note\n",
    "If there are both linear equality and inequality constraints, the projection matrix does not remain the same \n",
    "* At each iteration, it includes only the equality and active inequality constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Active set method\n",
    "Consider a problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }Ax\\leq b,\n",
    "$$\n",
    "where $A$ is a $l\\times n$ matrix ($l\\leq n$) and $b$ is a vector.\n",
    "\n",
    "## Idea\n",
    "* In $𝑥^ℎ$, the set of constraints is divided into active ($𝑖 ∈ 𝐼$) and inactive constraints\n",
    "* Inactive constraints are not taken into account when the search direction $𝑑^ℎ$ is determined\n",
    "* Inactive constraints affect only when computing the optimal step length $\\alpha^ℎ$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feasible directions\n",
    "* For $𝑖\\in 𝐼$ , $(𝑎^i)^Tx^h = b_i$\n",
    "* If $𝑑^ℎ$ is feasible in $𝑥^ℎ$, then $𝑥^ℎ + \\alpha 𝑑^ℎ \\in 𝑆$ for some $\\alpha > 0$\n",
    "* $(𝑎^i)^T(x^h+\\alpha d^h) = (a^i)^Tx^h + \\alpha(a^i)^Td^h\\leq b_i$\n",
    "* $(𝑎^i)^Td^h\\leq 0$ for feasible $𝑑^ℎ$ and the constraint remains active if $(𝑎^i)^Td^h=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On active constraints\n",
    "* Optimization problem with inequality constraints is more difficult than problem with equality constraints since the active set in a local minimizer is not known\n",
    "* If it would be known, then it would be enough to solve a corresponding equality constrained problem\n",
    "* In that case, if the other constraints would be satisfied in the solution and all the Lagrange multipliers were non-negative, then the solution would also be a solution to the original problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Using active set\n",
    "* At each iteration, a working set is considered which consists of the active constraints in $𝑥^ℎ$\n",
    "* The direction $𝑑^ℎ$ is determined so that it is a descent direction in the working set\n",
    "  * E.g. Rosen’s projected gradient method can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Active set algorithm\n",
    "1. Choose a starting point $x^1$ and determine an initial active set $I^1$ and set $h=1$\n",
    "2. Compute a feasible descent direction $d^h$ in the subspace defined by the active constraints (e.g. by using projected gradient)\n",
    "3. If $||d^h||=0$, go to step 6, otherwise, find optimal step length $\\alpha$ by staying in the feasible set and set $x^{h+1} = x^h + \\alpha d^h$\n",
    "4. If no new constraint becomes active go to step 7 (active set does not change)\n",
    "5. Addition to active set: a new constraint $j$ becomes active, update $I^{h+1} = I^h \\cup \\{j\\}$ and go to step 7\n",
    "6. Removal from active set: approximate Lagrangian multipliers $\\mu_i$, $i\\in I^h$. If $\\mu_i\\geq 0$ for all $i$, stop (active set is correct). Otherwise, remove a constraint $j$ with negative multiplier from the active set: $I^{h+1}=I^h\\setminus \\{j\\}$\n",
    "7. Set $h=h+1$ and go to step 2\n",
    "\n",
    "<i>Implementation of the active set method is left as a voluntary exercise </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More information about alternative way of computing the projected gradient and approximation for the Lagrange multipliers can be found in\n",
    "* <a href=\"http://users.jyu.fi/~jhaka/opt/TIES483_constrained_direct1.pdf\">Constrained optimization: direct methods (Part 1)</a>\n",
    "* <a href=\"http://users.jyu.fi/~jhaka/opt/TIES483_constrained_direct2.pdf\">Constrained optimization: direct methods (Part 2)</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
