{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Methods using KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reminder: optimality conditions\n",
    "We consider an optimization problem of the form:\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Necessary KKT conditions require for an optimal solution to satisfy\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x^*,\\mu^*,\\lambda^*) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J\\\\\n",
    "&\\mu_j^*g_j(x^*)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mu^*=(\\mu_1^*,\\ldots,\\mu_J^*)$ and $\\lambda^* = (\\lambda^*_1,\\ldots,\\lambda_K^*)$ are called Lagrance multiplier vectors for the inequality and the equality constraints, respectively. \n",
    "\n",
    "Function $L$ is the *Lagrangian function* $$L(x,\\mu,\\lambda) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lagrangian methods -- \"The original method of multipliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again consider problem\n",
    "\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "\n",
    "where the objective function and the equality constraints are twice differentiable. Inequality constaints can be handled e.g. by first converting them into equality constraints which increases the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that if $x^*$ is optimal solution, then $\\nabla_x L(x^*,\\lambda^*) = \\nabla f(x^*) + \\sum_{k=1}^K\\lambda^*_k \\nabla h_k(x^*) = 0$ \n",
    "(<i>necessary condition</i>). \n",
    "\n",
    "<b>However</b>, if we only know that for $x^*$ it holds that $\\nabla_x L(x^*,\\lambda^*) = 0$, we can't be sure that $x^*$ is a local minimizer. (Why???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the Hessian $\\nabla^2_{xx}L(x^*,\\lambda^*)$ may be indefinitive, it is not sufficient to just minimize the Langrangian function $L(x,\\lambda)$ in order to minimize $f(x)$ with respect to the equality constraints $h_k(x)=0$.\n",
    "\n",
    "<i>Solution: Improve Lagrangian function</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Define augmented Lagrangian function\n",
    "$$\n",
    "L_c(x,\\lambda) = f(x)+\\lambda h(x)+\\frac12c\\|h(x)\\|^2.\n",
    "$$\n",
    "Above $c\\in \\mathbb R$ is a penalty parameter and $\\lambda \\in \\mathbb R^K$ is a multiplier.\n",
    "\n",
    "Now, we have\n",
    "$$\n",
    "\\nabla^2_{xx}L_c(x^*,\\lambda^*) = \\nabla^2_{xx}L(x^*,\\lambda^*) + c\\nabla h(x^*)^T\\nabla h(x^*)\n",
    "$$\n",
    "and it can be shown that for $c>\\hat{c}$ the Hessian of the augmented Lagrangian is positive definite (<i>sufficient condition for optimality</i>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider sequence of optimization problems\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} f(x)+\\lambda_k h(x)+\\frac{1}{2}c_k\\|h(x)\\|^2,\n",
    "$$\n",
    "where $c_{k+1}>c_k$ for $k=1,2,\\ldots$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if $\\lambda_k=0$ for all $k=1,2,\\ldots$, then we have a penalty function method, which solves the problem when $c_k\\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be shown, that if we set $\\lambda_0$ randomly and keep on updating\n",
    "$\\lambda_{k+1} = \\lambda_k+c_kh(x_k)$, then we can show that there exists $C>0$ such that of $c_k>C$, then the optimal solution of the augmented Langrangian solves the original problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have optimization problem\n",
    "$$\n",
    "\\min x_1^2+x_2^2\\\\\n",
    "\\text{s.t. }x_1+x_2-1=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the minimization of the augmented Lagrangian becomes\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} x_1^2+x_2^2+\\lambda_k(x_1+x_2-1)+\\frac12c_k(x_1+x_2-1)^2.\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_constrained2(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_langrangian(f,x,la,c):\n",
    "    second_term = float(numpy.matrix(la)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]+second_term+third_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def augmented_langrangian_method(f,start,la0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    la = la0\n",
    "    c = c0\n",
    "    steps = []\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001: # doesn't work as itself, see starting from any feasible point\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,la,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        la = float(la+numpy.matrix(c)*numpy.matrix(f(res.x)[2]).transpose()) # update Lagrangian\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c # increase the penalty coefficient\n",
    "        steps.append(list(x_new))\n",
    "    return x_new,c, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x0 =[1/3,2/3]\n",
    "#x0 =[10,-5]\n",
    "l0 = 1.0\n",
    "c0 = 1.0\n",
    "[x,c,steps_ag] = augmented_langrangian_method(f_constrained2,x0,l0,c0)\n",
    "print(x)\n",
    "print(c)\n",
    "print(len(steps_ag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_2d_steps2(steps,start,interval):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'rx')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    # plot constraint\n",
    "    z = np.arange(interval[0],interval[1],0.1)\n",
    "    l = len(z)\n",
    "    con = [1.0-z[i] for i in range(l)]\n",
    "    plt.plot(z,con,'b-')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps2(steps_ag,x0,[0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compare with penalty function method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def penalty_function_method(f,start,c0):\n",
    "    x_old = float('inf')\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    c = c0\n",
    "    steps = []\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001:\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,0,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c\n",
    "        steps.append(list(x_new))\n",
    "    return x_new,c,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "[x,c,steps_pf] = penalty_function_method(f_constrained2,x0,c0)\n",
    "print(x)\n",
    "print(c)\n",
    "print(len(steps_pf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps2(steps_pf,x0,[0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is going on in here?\n",
    "\n",
    "The above is a simplified representation of the augmented Lagrangian method. For example, one can use exact second derivatives for calculating $\\nabla^2_{xx}L(x^*,\\mu^*)$ to obtain better convergence but, also, one can approximate it by utilizing ideas from quasi-Newton methods in order to not requiring second derivatives. Efficient implementation of this (and other methods) for practical problems is not completely trivial, unfortunately. If you want to read details, please see e.g., http://www.mit.edu/~dimitrib/Constrained-Opt.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Compare with SQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f,x,l,k):\n",
    "    #Define the lagrangian for given m and f\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2])*np.matrix(l).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y:\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,l):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "    np.matrix(diff_L(f,x,l,1)),\\\n",
    "    np.matrix(grad_h(f,x)).transpose()),axis=1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "    np.matrix(grad_h(f,x)),\\\n",
    "    np.matrix(np.zeros((len(f(x)[2]),len(f(x)[2]))))),axis=1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "    -1*np.matrix(diff_L(f,x,l,0)).transpose(),\n",
    "    -np.matrix(f(x)[2]).transpose()),axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row),axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side,right_hand_side)\n",
    "    return temp[:len(x)],temp[len(x):]\n",
    "    \n",
    "    \n",
    "\n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    l = np.ones(len(f(x)[2]))\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        print(x)\n",
    "        f_old = f_new\n",
    "        (p,v) = solve_QP(f,x,l)\n",
    "        x = x+np.array(p.transpose())[0]\n",
    "        l = l+v\n",
    "        f_new = f(x)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#x0 = [-3000,2500]\n",
    "SQP(f_constrained2,x0,0.00001)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
